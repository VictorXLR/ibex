#!/usr/bin/env python3
"""
Test script to demonstrate enhanced IBEX analysis functionality
"""

import asyncio
import sys
from pathlib import Path

# Add the python directory to the path
sys.path.insert(0, str(Path(__file__).parent / "python"))

from ibex.ai.self_monitor import IBEXSelfMonitor
from ibex.ai import AIManager

async def test_enhanced_analysis():
    """Test the enhanced analysis functionality"""
    print("ğŸ§ª Testing Enhanced IBEX Analysis Functionality")
    print("=" * 60)

    try:
        # Initialize the self-monitor
        print("ğŸ”§ Initializing IBEX Self-Monitor...")
        monitor = IBEXSelfMonitor()

        # Test the enhanced analysis
        print("ğŸ” Running enhanced analysis...")
        result = await monitor.analyze_recent_changes()

        print(f"ğŸ“Š Analysis Status: {result.get('status', 'Unknown')}")
        print(f"ğŸ“ Files Analyzed: {result.get('files_analyzed', 0)}")
        print(f"ğŸ¯ Project Intent: {result.get('project_intent', 'Not specified')}")
        print(f"ğŸ”¬ Analysis Depth: {result.get('analysis_depth', 'Basic')}")

        if result['status'] == 'analyzed':
            analysis = result['analysis']

            print(f"\nâ­ Quality Score: {analysis.get('quality_score', 0)}/10")

            print("\nğŸ“ Categories:")
            for category, files in analysis.get('categories', {}).items():
                print(f"  â€¢ {category.title()}: {len(files)} files")

            print("\nğŸ” Basic Feedback:")
            for feedback in analysis.get('feedback', []):
                print(f"  â€¢ {feedback}")

            print("\nğŸ’¡ Suggestions:")
            for suggestion in analysis.get('suggestions', []):
                print(f"  â€¢ {suggestion}")

            # Enhanced AI analysis
            ai_analysis = analysis.get('ai_analysis', {})
            if 'analysis' in ai_analysis:
                print(f"\nğŸ¤– AI Analysis ({ai_analysis.get('provider', 'unknown')}):")
                analysis_text = ai_analysis['analysis']
                if len(analysis_text) > 500:
                    print(f"  {analysis_text[:500]}...")
                else:
                    print(f"  {analysis_text}")

            # Improvements
            improvements = result.get('improvements', [])
            if improvements:
                print("\nğŸš€ Improvement Suggestions:")
                for improvement in improvements:
                    print(f"  â€¢ {improvement}")

            # Capabilities
            capabilities = result.get('ai_capabilities', {})
            if capabilities:
                print("\nâš¡ Analysis Capabilities Used:")
                for capability, enabled in capabilities.items():
                    status = "âœ…" if enabled else "âŒ"
                    print(f"  {status} {capability.replace('_', ' ').title()}")

        elif result['status'] == 'no_changes':
            print("ğŸ“ No uncommitted changes found to analyze")
            print("ğŸ’¡ Try making some changes to files and running this test again")

        elif result['status'] == 'error':
            print(f"âŒ Analysis Error: {result.get('message', 'Unknown error')}")

        print("\n" + "=" * 60)
        print("âœ… Enhanced analysis test completed!")

    except Exception as e:
        print(f"âŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()

async def test_ai_manager_capabilities():
    """Test AI manager enhanced capabilities"""
    print("\nğŸ§ª Testing AI Manager Enhanced Capabilities")
    print("=" * 60)

    try:
        # Test AI manager initialization
        print("ğŸ”§ Testing AI Manager initialization...")
        ai_manager = AIManager()

        print(f"ğŸ¤– Provider: {ai_manager.provider}")
        print(f"ğŸ§  Model: {ai_manager.model}")
        print(f"ğŸ“ Project Root: {ai_manager.project_root}")

        # Test file reading capability
        print("\nğŸ“– Testing file reading capability...")
        test_files = [
            "python/ibex/ai/__init__.py",
            "python/ibex/core.py",
            "README.md"
        ]

        for file_path in test_files:
            content = ai_manager.read_file_content(file_path, max_lines=10)
            if content.startswith("File not found"):
                print(f"  âŒ {file_path}: Not found")
            else:
                lines = content.count('\n')
                print(f"  âœ… {file_path}: {lines} lines read")

        # Test enhanced context creation
        print("\nğŸ”¬ Testing enhanced context creation...")
        context = ai_manager.create_enhanced_context(test_files[:2], "test")
        print(f"  ğŸ“ Context created: {len(context)} characters")
        print(f"  ğŸ“‹ Preview: {context[:200]}...")

        # Test system prompt creation
        print("\nğŸ’¬ Testing system prompt creation...")
        system_prompt = ai_manager.create_system_prompt("test", True)
        print(f"  ğŸ“ System prompt: {len(system_prompt)} characters")
        print(f"  ğŸ“‹ Preview: {system_prompt[:200]}...")

        print("\nâœ… AI Manager capabilities test completed!")

    except Exception as e:
        print(f"âŒ AI Manager test failed: {e}")
        import traceback
        traceback.print_exc()

async def main():
    """Run all tests"""
    print("ğŸš€ IBEX Enhanced Analysis Test Suite")
    print("=" * 60)

    await test_enhanced_analysis()
    await test_ai_manager_capabilities()

    print("\nğŸ‰ All tests completed!")
    print("\nğŸ’¡ To see the enhanced analysis in action:")
    print("  1. Make some changes to IBEX files")
    print("  2. Run: python run_ibex.py ai analyze-contribution")
    print("  3. Or use CLI: python run_ibex.py ai self-monitor")

if __name__ == "__main__":
    asyncio.run(main())
